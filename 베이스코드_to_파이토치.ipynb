{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "베이스코드_to_파이토치.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMjE5xsSRP1eo2XJQt6ykjo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junhyeongShin/colab/blob/main/%EB%B2%A0%EC%9D%B4%EC%8A%A4%EC%BD%94%EB%93%9C_to_%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdWzRSPXU4ap"
      },
      "source": [
        "# 런타임 TPU로 변경하고 사용하기!!!!!\n",
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl\n",
        "\n",
        "VERSION = \"20200325\"\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpxeuDm_VBeq"
      },
      "source": [
        "pip install torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YYaFPtIVDmV"
      },
      "source": [
        "# PyTorch에서는 .to(device) 문법을 통해 텐서 변수들과 모델들을 GPU와 같은 device에 올릴 수 있다.\n",
        "# TPU에 올리기 위해서는 torch_xla 에서 제공하는 xm.xla_device() 를 통해 PyTorch에 호환되는 device 를 지정할 수 있다.\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch\n",
        "\n",
        "\n",
        "#TPU 사용시 꼭 필요한 코드!!!!!!!!!!!!!!\n",
        "#xm.optimizer_step(optimizer, barrier=True)\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import Path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtCYxvucVDof"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/My\\ Drive/Colab Notebooks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIwk2bEkVQ7o"
      },
      "source": [
        "pip install Pillow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPaHJB7rVKX-"
      },
      "source": [
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxv74VjFVKaH"
      },
      "source": [
        "data_path = './data/'\n",
        "train_data_path = os.path.join(data_path, \"train\")\n",
        "print(train_data_path)\n",
        "file_list = os.listdir(train_data_path)\n",
        "file_list.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lzEHsyHaUHB"
      },
      "source": [
        "pip install torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wybJIAEEaSDh"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "from torchvision import transforms, utils\n",
        "from torchsummary import summary\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbFqfO56Wa1X"
      },
      "source": [
        "path = data_path\n",
        "train_csv = pd.read_csv(path+'train.csv')\n",
        "train_csv.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "722xvaEBa5Rl"
      },
      "source": [
        "train_path = sorted(glob(path+'train/*.npy'))\n",
        "train_path = train_path[-30*12:]\n",
        "\n",
        "train = []\n",
        "for p in tqdm(train_path):\n",
        "    # train.append(np.load(p))\n",
        "    tmp = np.load(p)\n",
        "    # tmp[:,:,0] = tmp[:,:,0]/250\n",
        "    train.append(tmp)\n",
        "    # (360, 448, 304, 5)\n",
        "train = np.array(train)\n",
        "train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlsOmp925zoZ"
      },
      "source": [
        "print(train[0][250][180][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dShL5dtna5T6"
      },
      "source": [
        "train_month = []\n",
        "for i in range(12):\n",
        "    train_month.append(train[i::12, :,:])\n",
        "train_month = np.array(train_month)\n",
        "\n",
        "train_month.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYQ-TBu-a5V7"
      },
      "source": [
        "stride = 32\n",
        "temp = np.zeros([train_month.shape[0],\n",
        "                 train_month.shape[1],\n",
        "                 train_month.shape[2],\n",
        "                 train_month.shape[3]+(stride-train_month.shape[3]%stride),\n",
        "                 train_month.shape[4]], np.uint8)\n",
        "temp[:, :,:train_month.shape[2], :train_month.shape[3], :] = train_month\n",
        "train_month = temp\n",
        "del temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVWWmEgcDYd2"
      },
      "source": [
        "window_size = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3r5W57S0DYgo"
      },
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "for i in range(train_month.shape[1]-window_size):\n",
        "    x_train.append(train_month[:, i:i+window_size, :,:,:])\n",
        "    y_train.append(train_month[:, i+window_size:i+window_size+1, :,:,:])\n",
        "    \n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BchvaHPvDYjb"
      },
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z261CkgwDYl2"
      },
      "source": [
        "#stride = 32\n",
        "feature_size = 128\n",
        "x_train_ = []\n",
        "y_train_ = []\n",
        "for y in range(x_train.shape[0]):\n",
        "    for m in range(12):\n",
        "        for i in range((x_train.shape[3]-feature_size)//stride+1):\n",
        "            for j in range((x_train.shape[4]-feature_size)//stride+1):\n",
        "                x_ = x_train[y, m, :, stride*i:stride*i+feature_size, stride*j:stride*j+feature_size, :1]\n",
        "                y_ = y_train[y, m, :, stride*i:stride*i+feature_size, stride*j:stride*j+feature_size, :1]\n",
        "                x_train_.append(x_)\n",
        "                y_train_.append(y_)\n",
        "\n",
        "x_train_ = np.array(x_train_)\n",
        "y_train_ = np.array(y_train_)\n",
        "\n",
        "x_val_ = []\n",
        "y_val_ = []\n",
        "\n",
        "for m in range(12):\n",
        "    for i in range((x_train.shape[3]-feature_size)//stride+1):\n",
        "        for j in range((x_train.shape[4]-feature_size)//stride+1):\n",
        "            x_ = x_train[-1, m, :, stride*i:stride*i+feature_size, stride*j:stride*j+feature_size, :1]\n",
        "            y_ = y_train[-1, m, :, stride*i:stride*i+feature_size, stride*j:stride*j+feature_size, :1]\n",
        "            x_val_.append(x_)\n",
        "            y_val_.append(y_)\n",
        "            \n",
        "x_val_ = np.array(x_val_)\n",
        "y_val_ = np.array(y_val_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32Yr5uWzDYoN"
      },
      "source": [
        "x_train_.shape, y_train_.shape, x_val_.shape, y_val_.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHOFmQk3DYqy"
      },
      "source": [
        "i = 38\n",
        "plt.figure(figsize=(15,10))\n",
        "for y in range(x_train_.shape[1]):\n",
        "    plt.subplot(1,window_size,y+1)\n",
        "    plt.imshow(x_train_[i,y,:,:,0])\n",
        "plt.show()\n",
        "plt.imshow(y_train_[i,-1,:,:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j653E_Zb2Oe2"
      },
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, train_x,train_y,batch_size,transform):\n",
        "        super(TrainDataset, self).__init__()\n",
        "\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.batch_size = batch_size\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        # 데이터 갯수\n",
        "        return self.batch_size\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        x = self.train_x[idx*self.batch_size : idx*self.batch_size + self.batch_size]\n",
        "        y = self.train_y[idx*self.batch_size : idx*self.batch_size + self.batch_size]\n",
        "\n",
        "        # x = self.train_x\n",
        "        # y = self.train_y\n",
        "\n",
        "        x = torch.tensor(x)\n",
        "        y = torch.tensor(y)\n",
        "\n",
        "        return x,y\n",
        "\n",
        "\n",
        "\n",
        "def getTransform():\n",
        "    return transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "transform = getTransform()\n",
        "\n",
        "#데이터 로더에서 자료 가져오고 배치 사이즈 적용하게끔 하셈\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataset = TrainDataset(x_train_,y_train_,BATCH_SIZE,transform)\n",
        "valid_dataset = TrainDataset(x_val_,y_val_,BATCH_SIZE,transform)\n",
        "\n",
        "# Dataloader 클래스는 데이터셋에서 배치 개수만큼 뽑아서 제공해줍니다\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, worker_init_fn=2, prefetch_factor=2)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, worker_init_fn=2, prefetch_factor=2)\n",
        "\n",
        "\n",
        "# show example\n",
        "# train_dataloader length => 32\n",
        "\n",
        "# show one of train_ds\n",
        "for x, y in train_dataset:\n",
        "    print(x.shape, y.shape)\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96n-9XR_2Q8S"
      },
      "source": [
        "m = nn.Conv2d(5,64,3,padding=1)\n",
        "\n",
        "input = train_dataset.train_x[:32,:5,:,:,0]\n",
        "\n",
        "input = torch.tensor(input)\n",
        "\n",
        "input = input.float()\n",
        "\n",
        "output = m(input)\n",
        "\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux8OFmrUxDYj"
      },
      "source": [
        "for x, y in train_dataset:\n",
        "    print(type(x))\n",
        "    print(type(y))\n",
        "    print(x.shape)\n",
        "    print(y.shape)\n",
        "\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRFxZuXM2Bn4"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "\n",
        "class ConvLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, bias):\n",
        "        \"\"\"\n",
        "        Initialize ConvLSTM cell.\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_dim: int\n",
        "            Number of channels of input tensor.\n",
        "        hidden_dim: int\n",
        "            Number of channels of hidden state.\n",
        "        kernel_size: (int, int)\n",
        "            Size of the convolutional kernel.\n",
        "        bias: bool\n",
        "            Whether or not to add the bias.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = kernel_size[0] // 2, kernel_size[1] // 2\n",
        "        self.bias = bias\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels=self.input_dim + self.hidden_dim,\n",
        "                              out_channels=4 * self.hidden_dim,\n",
        "                              kernel_size=self.kernel_size,\n",
        "                              padding=self.padding,\n",
        "                              bias=self.bias)\n",
        "\n",
        "    def forward(self, input_tensor, cur_state):\n",
        "        h_cur, c_cur = cur_state\n",
        "\n",
        "        combined = torch.cat([input_tensor, h_cur], dim=1)  # concatenate along channel axis\n",
        "\n",
        "        combined_conv = self.conv(combined)\n",
        "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "    def init_hidden(self, batch_size, image_size):\n",
        "        height, width = image_size\n",
        "        return (torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device),\n",
        "                torch.zeros(batch_size, self.hidden_dim, height, width, device=self.conv.weight.device))\n",
        "\n",
        "\n",
        "class ConvLSTM(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "        input_dim: Number of channels in input\n",
        "        hidden_dim: Number of hidden channels\n",
        "        kernel_size: Size of kernel in convolutions\n",
        "        num_layers: Number of LSTM layers stacked on each other\n",
        "        batch_first: Whether or not dimension 0 is the batch or not\n",
        "        bias: Bias or no bias in Convolution\n",
        "        return_all_layers: Return the list of computations for all layers\n",
        "        Note: Will do same padding.\n",
        "    Input:\n",
        "        A tensor of size B, T, C, H, W or T, B, C, H, W\n",
        "    Output:\n",
        "        A tuple of two lists of length num_layers (or length 1 if return_all_layers is False).\n",
        "            0 - layer_output_list is the list of lists of length T of each output\n",
        "            1 - last_state_list is the list of last states\n",
        "                    each element of the list is a tuple (h, c) for hidden state and memory\n",
        "    Example:\n",
        "        >> x = torch.rand((32, 10, 64, 128, 128))\n",
        "        >> convlstm = ConvLSTM(64, 16, 3, 1, True, True, False)\n",
        "        >> _, last_states = convlstm(x)\n",
        "        >> h = last_states[0][0]  # 0 for layer index, 0 for h index\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, kernel_size, num_layers,\n",
        "                 batch_first=False, bias=True, return_all_layers=False):\n",
        "        super(ConvLSTM, self).__init__()\n",
        "\n",
        "        self._check_kernel_size_consistency(kernel_size)\n",
        "\n",
        "        # Make sure that both `kernel_size` and `hidden_dim` are lists having len == num_layers\n",
        "        kernel_size = self._extend_for_multilayer(kernel_size, num_layers)\n",
        "        hidden_dim = self._extend_for_multilayer(hidden_dim, num_layers)\n",
        "        if not len(kernel_size) == len(hidden_dim) == num_layers:\n",
        "            raise ValueError('Inconsistent list length.')\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "        self.bias = bias\n",
        "        self.return_all_layers = return_all_layers\n",
        "\n",
        "        cell_list = []\n",
        "        for i in range(0, self.num_layers):\n",
        "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dim[i - 1]\n",
        "\n",
        "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
        "                                          hidden_dim=self.hidden_dim[i],\n",
        "                                          kernel_size=self.kernel_size[i],\n",
        "                                          bias=self.bias))\n",
        "\n",
        "        self.cell_list = nn.ModuleList(cell_list)\n",
        "\n",
        "    def forward(self, input_tensor, hidden_state=None):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_tensor: todo\n",
        "            5-D Tensor either of shape (t, b, c, h, w) or (b, t, c, h, w)\n",
        "        hidden_state: todo\n",
        "            None. todo implement stateful\n",
        "        Returns\n",
        "        -------\n",
        "        last_state_list, layer_output\n",
        "        \"\"\"\n",
        "        if not self.batch_first:\n",
        "            # (t, b, c, h, w) -> (b, t, c, h, w)\n",
        "            input_tensor = input_tensor.permute(1, 0, 2, 3, 4)\n",
        "\n",
        "        b, _, _, h, w = input_tensor.size()\n",
        "\n",
        "        # Implement stateful ConvLSTM\n",
        "        if hidden_state is not None:\n",
        "            raise NotImplementedError()\n",
        "        else:\n",
        "            # Since the init is done in forward. Can send image size here\n",
        "            hidden_state = self._init_hidden(batch_size=b,\n",
        "                                             image_size=(h, w))\n",
        "\n",
        "        layer_output_list = []\n",
        "        last_state_list = []\n",
        "\n",
        "        seq_len = input_tensor.size(1)\n",
        "        cur_layer_input = input_tensor\n",
        "\n",
        "        for layer_idx in range(self.num_layers):\n",
        "\n",
        "            h, c = hidden_state[layer_idx]\n",
        "            output_inner = []\n",
        "            for t in range(seq_len):\n",
        "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :, :],\n",
        "                                                 cur_state=[h, c])\n",
        "                output_inner.append(h)\n",
        "\n",
        "            layer_output = torch.stack(output_inner, dim=1)\n",
        "            cur_layer_input = layer_output\n",
        "\n",
        "            layer_output_list.append(layer_output)\n",
        "            last_state_list.append([h, c])\n",
        "\n",
        "        if not self.return_all_layers:\n",
        "            layer_output_list = layer_output_list[-1:]\n",
        "            last_state_list = last_state_list[-1:]\n",
        "\n",
        "        return layer_output_list, last_state_list\n",
        "\n",
        "    def _init_hidden(self, batch_size, image_size):\n",
        "        init_states = []\n",
        "        for i in range(self.num_layers):\n",
        "            init_states.append(self.cell_list[i].init_hidden(batch_size, image_size))\n",
        "        return init_states\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_kernel_size_consistency(kernel_size):\n",
        "        if not (isinstance(kernel_size, tuple) or\n",
        "                (isinstance(kernel_size, list) and all([isinstance(elem, tuple) for elem in kernel_size]))):\n",
        "            raise ValueError('`kernel_size` must be tuple or list of tuples')\n",
        "\n",
        "    @staticmethod\n",
        "    def _extend_for_multilayer(param, num_layers):\n",
        "        if not isinstance(param, list):\n",
        "            param = [param] * num_layers\n",
        "        return param\n",
        "\n",
        "\n",
        "\n",
        "# The ConvLSTM module derives from nn.Module so it can be used as any other PyTorch module.\n",
        "\n",
        "# The ConvLSTM class supports an arbitrary number of layers. \n",
        "# In this case, it can be specified the hidden dimension (that is, the number of channels) and the kernel size of each layer. \n",
        "# In the case more layers are present but a single value is provided, this is replicated for all the layers. \n",
        "\n",
        "# For example, in the following snippet each of the three layers has a different hidden dimension but the same kernel size.\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "# model = ConvLSTM(input_dim=channels,\n",
        "#                  hidden_dim=[64, 64, 128],\n",
        "#                  kernel_size=(3, 3),\n",
        "#                  num_layers=3,\n",
        "#                  batch_first=True\n",
        "#                  bias=True,\n",
        "#                  return_all_layers=False)        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH5vqjBOSXtm"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "MODEL_PARAMS = {\n",
        "    \"shape\" : (5, 1, 128, 128),\n",
        "    \"init_filters\": 64,\n",
        "    \"dropout_rate\" : 0.5\n",
        "}\n",
        "\n",
        "class Net(nn.Module): # nn.Module 모든 신경망 모듈의 기본이 되는 클래스\n",
        "                      # 각 층과 함수 등 신경망의 구성요소를 이 클래스 안에서 정의한다.\n",
        "                      # nn.Module은 모든 신경망 모듈의 기본이 되는 클래스로 레이어, 함수등을 정의하는구나!\n",
        "\n",
        "    def __init__(self):  # 초기화 함수   \n",
        "        super(Net, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 3x3 square convolution kernel\n",
        "        # (kernel과 filter는 같다) 즉, filter size는 3x3\n",
        "\n",
        "        input_frames, input_channel, input_height, input_width = params[\"shape\"] # input_frames? input_batch?\n",
        "        init_filters = params[\"init_filters\"]\n",
        "        self.dropout_rate = params[\"dropout_rate\"]\n",
        "\n",
        "        #RNN 모델 적용 필요하다\n",
        "        self.conv1 = nn.Conv2d(input_channel, 64, 64,padding=1) # 입력 채널 수, 출력 채널 수, 필터의 크기   \n",
        "        self.batchnorm1 = nn.BatchNorm2d() \n",
        "        self.conv2 = nn.Conv2d(64, 64*64, 64) # 입력 채널 수, 출력 채널 수, 필터의 크기    \n",
        "        self.batchnorm2 = nn.BatchNorm2d()\n",
        "        self.conv3 = nn.Conv2d(64*64, 64*64*3, 3) # 입력 채널 수, 출력 채널 수, 필터의 크기   \n",
        "        self.batchnorm3 = nn.BatchNorm2d() \n",
        "\n",
        "        self.conv4 = nn.Conv3d(input_channel, 1, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = conv1(x)\n",
        "        x = batchnorm1(x)\n",
        "        x = conv2(x)\n",
        "        x = batchnorm2(x)        \n",
        "        x = conv3(x)\n",
        "        x = batchnorm3(x)\n",
        "        x = conv4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Creating Model\n",
        "\n",
        "class CustomNet(nn.Module):\n",
        "    def __init__(self, params):\n",
        "        super(CustomNet, self).__init__()\n",
        "        input_frames, input_channel, input_height, input_width = params[\"shape\"] # input_frames? input_batch?\n",
        "        init_filters = params[\"init_filters\"]\n",
        "        self.dropout_rate = params[\"dropout_rate\"]\n",
        "        self.conv1 = nn.Conv3d(input_channel, init_filters, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv3d(init_filters, init_filters*2, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.ConvTranspose3d(init_filters*2, 1, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool3d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.upsample(x, size=(2, 128, 128))\n",
        "        print(\"input: \", input.shape)\n",
        "        print(\"output: \", x.shape)\n",
        "        return x\n",
        "\n",
        "class test_Net(nn.Module): # nn.Module 모든 신경망 모듈의 기본이 되는 클래스\n",
        "                      # 각 층과 함수 등 신경망의 구성요소를 이 클래스 안에서 정의한다.\n",
        "                      # nn.Module은 모든 신경망 모듈의 기본이 되는 클래스로 레이어, 함수등을 정의하는구나!\n",
        "\n",
        "    def __init__(self):  # 초기화 함수   \n",
        "        super(test_Net, self).__init__()\n",
        "\n",
        "        #RNN 모델 적용 필요하다\n",
        "        self.conv1 = nn.Conv2d(5,64*5, 3,padding=1) # 입력 채널 수, 출력 채널 수, 필터의 크기  \n",
        "         \n",
        "        # self.lstm1 = nn.LSTM(64,hidden_size=1,batch_first=True)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(64*5) \n",
        "        # self.conv4 = nn.Conv3d(64, 1, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        # x = self.lstm1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        # x = self.conv4(x)\n",
        "\n",
        "        return x        \n",
        "\n",
        "\n",
        "# net = Net()\n",
        "# print(net)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpxv7u7z1FYm"
      },
      "source": [
        "class test_Net(nn.Module): # nn.Module 모든 신경망 모듈의 기본이 되는 클래스\n",
        "                      # 각 층과 함수 등 신경망의 구성요소를 이 클래스 안에서 정의한다.\n",
        "                      # nn.Module은 모든 신경망 모듈의 기본이 되는 클래스로 레이어, 함수등을 정의하는구나!\n",
        "\n",
        "    def __init__(self):  # 초기화 함수   \n",
        "        super(test_Net, self).__init__()\n",
        "\n",
        "        #RNN 모델 적용 필요하다\n",
        "        self.conv1 = nn.Conv2d(1,64, 3,padding=1) # 입력 채널 수, 출력 채널 수, 필터의 크기   \n",
        "        # self.lstm1 = nn.LSTM(64,hidden_size=1,batch_first=True)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(64) \n",
        "        # self.conv4 = nn.Conv3d(64, 1, kernel_size=3, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        # x = self.lstm1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        # x = self.conv4(x)\n",
        "\n",
        "        return x "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjNqLK_DEp4I"
      },
      "source": [
        "model = test_Net()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6KLvDZiEszf"
      },
      "source": [
        "nb_epochs = 20\n",
        "for epoch in range(nb_epochs + 1):\n",
        "  for batch_idx, samples in enumerate(train_dataloader):\n",
        "    # print(batch_idx)\n",
        "    # print(len(samples))\n",
        "    x_train[:32,:], y_train[:32,:] = samples\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train)\n",
        "\n",
        "    # cost 계산\n",
        "    cost = F.mse_loss(prediction, y_train)\n",
        "\n",
        "    # cost로 H(x) 계산\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\n",
        "        epoch, nb_epochs, batch_idx+1, len(dataloader),\n",
        "        cost.item()\n",
        "        ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKssCH4zpwBM"
      },
      "source": [
        "#TPU 불러오기\n",
        "device = xm.xla_device()\n",
        "my_model = test_Net()\n",
        "\n",
        "# my_model = ConvLSTM(1,hidden_dim=[64, 64, 128],\n",
        "#                  kernel_size=(3, 3),\n",
        "#                  num_layers=3,\n",
        "#                  batch_first=True,\n",
        "#                  bias=True,\n",
        "#                  return_all_layers=False) # to(device) 해줘야 에러가 안남\n",
        "print(my_model)\n",
        "# 32, 32, 5, 128, 128, 1\n",
        "my_model.summary()\n",
        "# summary(my_model, input_size=(32, 5, 128, 128)) # summary 함수를 통해 임의의 사이즈를 넣어 구조와 파라미터를 확인할 수 있습니다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-6tIfl1xAtL"
      },
      "source": [
        "\n",
        "\n",
        "#모델 생성\n",
        "net = test_Net()\n",
        "#TPU 불러오기\n",
        "device = xm.xla_device()\n",
        "#모델에 TPU 사용하기\n",
        "# net.to(device) # to(device) 해줘야 에러가 안남\n",
        "\n",
        "print(net)\n",
        "# summary 함수를 통해 임의의 사이즈를 넣어 구조와 파라미터를 확인할 수 있습니다\n",
        "summary(net, input_size=(1, 128, 128),batch_size=32) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akZ1SYxcDsXP"
      },
      "source": [
        "\n",
        "\n",
        "#     convLSTM = BatchNormalization()(convLSTM)\n",
        "\n",
        "\n",
        "\n",
        "#     convLSTM = ConvLSTM2D(filters=64, kernel_size=3, padding='same', return_sequences=True)(convLSTM)\n",
        "\n",
        "#return_sequences=True의 의미는 \n",
        "#LSTM의 중간 스텝의 출력을 모두 사용하라는 의미다. \n",
        "#그리고 TimeDistributed() 함수의 의미는 각 스텝마다 cost (오류)를 계산해서 하위 스텝으로 \n",
        "#오류를 전파하여 각 weight를 업데이트하라는 의미다. \n",
        "\n",
        "\n",
        "#     convLSTM = BatchNormalization()(convLSTM)\n",
        "\n",
        "#     convLSTM = ConvLSTM2D(filters=64, kernel_size=3, padding='same', return_sequences=True)(convLSTM)\n",
        "#     convLSTM = BatchNormalization()(convLSTM)\n",
        "\n",
        "#     outputs = Conv3D(filters=1, kernel_size=3, activation='relu', padding='same', data_format='channels_last')(convLSTM)\n",
        "\n",
        "#     model = Model(inputs, outputs)\n",
        "#     model.compile(loss='mae', optimizer=Adam())\n",
        "\n",
        "\n",
        "    \n",
        "# model.summary()\n",
        "\n",
        "# Model: \"model\"\n",
        "# _________________________________________________________________\n",
        "# Layer (type)                 Output Shape              Param #   \n",
        "# =================================================================\n",
        "# input_1 (InputLayer)         [(None, None, 128, 128, 1 0         \n",
        "# _________________________________________________________________\n",
        "# conv_lst_m2d (ConvLSTM2D)    (None, None, 128, 128, 64 150016    \n",
        "# _________________________________________________________________\n",
        "# batch_normalization (BatchNo (None, None, 128, 128, 64 256       \n",
        "# _________________________________________________________________\n",
        "# conv_lst_m2d_1 (ConvLSTM2D)  (None, None, 128, 128, 64 295168    \n",
        "# _________________________________________________________________\n",
        "# batch_normalization_1 (Batch (None, None, 128, 128, 64 256       \n",
        "# _________________________________________________________________\n",
        "# conv_lst_m2d_2 (ConvLSTM2D)  (None, None, 128, 128, 64 295168    \n",
        "# _________________________________________________________________\n",
        "# batch_normalization_2 (Batch (None, None, 128, 128, 64 256       \n",
        "# _________________________________________________________________\n",
        "# conv3d (Conv3D)              (None, None, 128, 128, 1) 1729      \n",
        "# =================================================================\n",
        "# Total params: 742,849\n",
        "# Trainable params: 742,465\n",
        "# Non-trainable params: 384\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji_2bTVaql7P"
      },
      "source": [
        "# Loss Function && etric Function\n",
        "\n",
        "# metrics\n",
        "def mae_score(true, pred):\n",
        "    true, pred = numpy_to_tensor(true, pred)\n",
        "    score = np.mean(np.abs(true-pred))\n",
        "    \n",
        "    return score\n",
        "\n",
        "# metrics\n",
        "def f1_score(true, pred):\n",
        "    true, pred = numpy_to_tensor(true, pred)\n",
        "\n",
        "    target = np.where((true>1*0.05)<1*0.5)\n",
        "    \n",
        "    true = true[target]\n",
        "    pred = pred[target]\n",
        "    true = np.where(true < 1*0.15, 0, 1)\n",
        "    pred = np.where(pred < 1*0.15, 0, 1)\n",
        "    \n",
        "    \n",
        "    right = np.sum(true * pred == 1)\n",
        "    precision = right / np.sum(true+1e-8)\n",
        "    recall = right / np.sum(pred+1e-8)\n",
        "    score = 2 * precision*recall/(precision+recall+1e-8)\n",
        "    \n",
        "    return score\n",
        "    \n",
        "# loss function\n",
        "def mae_over_f1(true, pred):\n",
        "    mae = mae_score(true, pred)\n",
        "    f1 = f1_score(true, pred)\n",
        "    score = mae/(f1+1e-8)\n",
        "    \n",
        "    return score\n",
        "\n",
        "def numpy_to_tensor(true, pred):\n",
        "    return true.cpu().detach().numpy(), pred.cpu().detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rYyMbP6DsZj"
      },
      "source": [
        "# Optimizer\n",
        "\n",
        "# adam optimizer\n",
        "opt_adam = optim.Adam(my_model.parameters(), lr=3e-4)\n",
        "\n",
        "def get_lr(opt):\n",
        "    for param_group in opt.param_groups:\n",
        "        return param_group[\"lr\"]\n",
        "\n",
        "# check our learning rate\n",
        "current_lr = get_lr(opt_adam)\n",
        "print(f\"current_lr = {current_lr}\")\n",
        "\n",
        "\n",
        "\n",
        "# learning rate scheduler\n",
        "lr_scheduler = ReduceLROnPlateau(opt_adam, mode=\"min\", factor=0.5, patience=20, verbose=1)\n",
        "\n",
        "# example \n",
        "for i in range(100):\n",
        "    lr_scheduler.step(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZyfGxXEDscA"
      },
      "source": [
        "# Training \n",
        "\n",
        "def metrics_batch(pred, true, metrics):\n",
        "    # if needed add param \"metrics\" to custom\n",
        "    \"\"\"\n",
        "    output will be pred\n",
        "    target will be corrects\n",
        "    \"\"\"\n",
        "    if metrics:\n",
        "        return list(map(lambda x: x(true, pred), metrics))\n",
        "    mae_score = mae_score(true, pred)\n",
        "    f1_score = f1_score(true, pred)\n",
        "    return (mae_score, f1_score)\n",
        "\n",
        "def loss_batch(loss_func, pred, true, opt=None):\n",
        "    \"\"\"\n",
        "    loss_func => mae_over_f1\n",
        "    \"\"\"\n",
        "    loss = loss_func(true, pred)\n",
        "    with torch.no_grad():\n",
        "        metrics = metrics_batch(pred, true, [mae_score, f1_score])\n",
        "    if opt is not None:\n",
        "        opt.zero_grad()\n",
        "        # loss.backward()\n",
        "        opt.step()  # 학습이 이뤄지는 곳\n",
        "    return loss, metrics\n",
        "\n",
        "def loss_epoch(model, loss_func, dataset_dataloader, sanity_check=False, opt=None):\n",
        "    running_loss = 0.0\n",
        "    running_metric = [0.0, 0.0]\n",
        "    len_data = len(dataset_dataloader.dataset)\n",
        "\n",
        "    for x, y in dataset_dataloader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        # 모델 결과\n",
        "        pred = model(x)\n",
        "        # 손실함수 구하기\n",
        "        loss, metrics = loss_batch(loss_func, pred, y, opt)\n",
        "        # 손실함수 \n",
        "        running_loss += loss\n",
        "        if metrics is not None:\n",
        "            for idx, metric_value in enumerate(metrics):\n",
        "                running_metric[idx] += metric_value\n",
        "        \n",
        "        # 문제 있으면 break, 여기서는 True 일때 바로 break\n",
        "        if sanity_check is True:\n",
        "            break\n",
        "    \n",
        "    loss = running_loss / float(len_data)\n",
        "    metrics = list(map(lambda x: x/float(len_data), metrics))\n",
        "    print(loss, metrics)\n",
        "    return loss, metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL6f978Bq4Vs"
      },
      "source": [
        "loss_func = mae_over_f1\n",
        "opt_adam = optim.Adam(my_model.parameters(), lr=3e-4)\n",
        "lr_scheduler = ReduceLROnPlateau(opt_adam, mode=\"min\", factor=0.5, patience=20, verbose=1)\n",
        "\n",
        "TRAIN_PARAMS = {\n",
        "    \"num_epochs\" : 10,\n",
        "    \"loss_func\" : loss_func,\n",
        "    \"optimizer\" : opt_adam,\n",
        "    \"train_dataloader\" : train_dataloader,\n",
        "    \"valid_dataloader\" : valid_dataloader,\n",
        "    \"sanity_check\" : True,\n",
        "    \"lr_scheduler\" : lr_scheduler,\n",
        "    \"save_path\" : \"./weights.pt\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYRYUBvxq4Ts"
      },
      "source": [
        "import copy\n",
        "\n",
        "def train(model, params):\n",
        "    num_epochs = params['num_epochs']\n",
        "    loss_func = params['loss_func']\n",
        "    opt = params[\"optimizer\"]\n",
        "    train_dataloader = params['train_dataloader']\n",
        "    valid_dataloader = params['valid_dataloader']\n",
        "    sanity_check = params['sanity_check']\n",
        "    lr_scheduler = params['lr_scheduler']\n",
        "    save_path = params['save_path']\n",
        "\n",
        "    # keep history of the loss and metric\n",
        "    loss_hist = {\n",
        "        \"train\" : [],\n",
        "        \"valid\" : []\n",
        "    }\n",
        "\n",
        "    metrics_hist = {\n",
        "        \"train\" : [],\n",
        "        \"valid\" : []\n",
        "    }\n",
        "\n",
        "    # copy best weights\n",
        "    best_model_weights = copy.deepcopy(model.state_dict())\n",
        "    # init best loss\n",
        "    best_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        current_lr = get_lr(opt)\n",
        "        print(f'Epoch:{epoch}/{num_epochs-1}, current lr:{current_lr}')\n",
        "        model.train()\n",
        "        train_loss, train_metrics = loss_epoch(model, loss_func, train_dataloader, sanity_check, opt)\n",
        "\n",
        "        # save history\n",
        "        loss_hist[\"train\"].append(train_loss)\n",
        "        metrics_hist[\"train\"].append(train_metrics)\n",
        "\n",
        "        # model.eval()\n",
        "        # with torch.no_grad():\n",
        "    \n",
        "\n",
        "    return model, loss_hist, metrics_hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzQVLK8pq4RM"
      },
      "source": [
        "my_model, loss_hist, metrics_hist = train(my_model, TRAIN_PARAMS)\n",
        "\n",
        "print(loss_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4PAE7Cgq4MY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}